import algorithms as algs
import semanticsky as ss

CLUES = []
AGENTS = []
_god = None

class Clue(object):
	
	def __init__(self,about,value,agent = 'god'):
		
		if isinstance(about,frozenset):
			self.cluetype = 'link' 
		elif about in algs.ALL_ALGS:
			self.cluetype = 'accuracy' # the clue is about an algorithm's accuracy
		elif isinstance(about,Clue): 
			self.cluetype = 'metaclue' # the clue is about the validity of another clue.			
		else:
			raise BaseException('Unrecognized about input: {}.'.format(about))	
			
		self.about = about
		self.value = value
		self.agent = agent
		
		global _god
		if agent is 'god' and _god is None:
			Agent('god') 
		
		CLUES.append(self)
		
	def __str__(self):
		
		translateclue = {	'link': 'the validity of a link',
							'accuracy': 'the accuracy of an algorithm',
							'metaclue' : 'the usefulness of another clue'}
							
		translatevalue = {		1 : 'accurate',
								-1 : 'inaccurate'}
		
		return "< Clue about {} ({}), which was evaluated '{}' by Agent {}.>".format(translateclue[self.cluetype],self.about,self.value,self.agent)
	
	@property
	def trustworthiness(self):
		"""
		The trustworthiness of a clue is the one of he who formulated it.
		"""
		return self.agent.stats['trustworthiness']
		
	def receive(self,clue):
		"""
		There also are clues about clues: if the outcome of a clue (the 
		modification of a weight) proves useful, clues can be spawned that
		rate the former clue valuable (or worthless): In this case, the 
		clue backpropagates to its agent (the author) the positive (or 
		negative) feedback.
		"""
		
		return self.agent.receive(clue)
		
class Agent(object):
	
	def __init__(self,name = 'Anonymous'):
		
		self.name = name
		self.clues = []
		self.stats = { 	'trustworthiness': 0.5,
						'expertises': [],
						'communities': [],
						'blocked' : False}
		
						
		AGENTS.append(self)
		if self.name == 'god':
			self.make_god()
	
	def __str__(self):
		return "< Agent {}.>".format(self.name)
	
	def make_god(self):
		
		global _god
		
		if _god is None:
			self = God()
			_god = self
			
	def evaluate(self,what,howmuch):
		"""
		Formulates a Clue about what, judging it howmuch.
		"""
		
		if self.stats['blocked']:
			return None
		
		if howmuch > 1:
			raise BaseException('Evaluation confidence should not be above one.')
			
		myclue = Clue(what,howmuch,self)
		self.clues.append(myclue)
		return myclue
	
	def receive(self,clue):
		"""
		An agent is the ultimate recipient of a clue: his own trustworthiness
		depends on received clues (that is: clues formulated by others or)
		automatically generated that rate its clues.
		"""
		
		self.stats['trustworthiness'] = ( self.stats['trustworthiness'] + clue.value ) / 2
	
	def suggest_link(self,link,confidence):
		"""
		A link must be of type frozenset({Cloud(),Cloud()}).
		The belief in that link's validity is clued +1 by the agent.
		"""
		error = False
		if not (isinstance(link, frozenset) and len(link) == 2):
			error = True 
		
		tuplelink = tuple(link)
		if not (isinstance(tuplelink[0],ss.Cloud) and isinstance(tuplelink[1],ss.Cloud)):
			error = True
		if error:
			raise BaseException('Bad link type: {} ; frozenset({Cloud(),Cloud()}) needed.'.format(type(link)))
			
		self.evaluate(link,confidence)

class GuardianAngels(Agent,object):
	"""
	A GuardianAngel is a bot: an agent whose decisions are generated by
	an algorithm.
	- GuardianAngel don't output clues at will; only when prompted to do so
	by God.
	- GuardianAngel can't reinforce or weaken each other: they can take feedback
	only on behalf of normal agents or god.
	"""
	
	def __init__(self,algorithm):
		super().__init__(algorithm.__name__)
		self.algorithm = algorithm
		self.stats['trustworthiness'] = 1 # by default, an algorithm's trustworthiness is always one.
		self.evaluation = {}
		self.clues = [] # Clues objects
	
	def __str__(self):
		return "< GuardianAngel {} >".format(self.name)
		
	def evaluate(self,what,silent = False):
		"""
		what must be a pair-of-clouds instance, for the moment.
		the Algorithm's algorithm.
		Is basically an Agent whose judgements are totally automatic.
		returns the clue.
		
		In silent mode, only the evaluation is returned. Useful for when
		god wants to choose between its GuardianAngel the best judgement
		before taking it into account.
		"""
		
		try:
			evaluation = self.algorithm(*what)
		except BaseException as e:
			print(what)
			raise e
			
		self.evaluation[what] = evaluation # updates the evaluation
		
		if silent:
			return None
		else:
			myclue = Clue(what,evaluation)
			self.clues.append(myclue)
			return myclue
	
	def evaluate_all(self,iterable_clouds):
		"""
		Tells the GuardianAngel to do a full evaluation: for each pair of clouds,
		formulates a clue about their relatedness.
		Then, asks God for a moment of attention to assess these evaluations.
		"""
				
		for clouda in iterable_clouds:
			i = 1
			for cloudb in iterable_clouds[i:]:
				if clouda is cloudb:
					continue
				pair = frozenset({clouda,cloudb})
				self.evaluate(pair,silent = True)
		
			i += 1
			
		
class God(Agent,object):
	"""
	The Allmighty.
	"""
	def __str__(self):
		return "< The Lord >"
	
	def receive(self,clue):
		
		print('God accepts no feedback, mortal.')
		return None
	
	def consider_clues(self):
		"""
		God will shot a quick glance to the useless complains of the mortals.
		"""
		
		global CLUES
		
		for clue in CLUES:
			if clue.cluetype in ['link','accuracy','metaclue']:
				handler = getattr(self, 'handle_{}'.format(clue.cluetype) )
				CLUES.remove(clue)
				handler(clue) # the handler will handle
			else:
				raise BaseException('Unrecognized cluetype: {}.'.format(clue.cluetype))
		
		CLUES = []
		
	def handle_metaclue(self,metaclue):
		"""
		This is a handler for clues about clues1: someone is complaining 
		that a clue1 was useless, or that it was very good.
		Propagate the clue to the agent of clue1.
		"""
		
		target_clue = metaclue.about # the clue which is rated by the metaclue
		return target_clue.receive(metaclue) # the metaclue's value will in the end average up or down the target_clue's author's trustworthiness
		
	def handle_link(self,linkclue):
		"""
		where linkclue is a clue about the existence of a link, ( or about
		the similarity of two clouds, if you wish, this function makes god's
		ineffable beliefs change (a bit) accordingly.
		
		God does listen at his Agents' complaints, but they'll have to scream 
		aloud enough.
		"""
		
		self.update_beliefs(linkclue)
	
	def update_beliefs(self,clue):
		"""
		Where clue is a clue about anything believable by god.
		"""
		
		try: 
			self.beliefs = self.beliefs		
		except AttributeError:
			self.beliefs = {}

		if not self.beliefs.get(clue.about,False):
			self.beliefs[clue.about] = 0 # the initial belief is zero: if asked 'do you believe x?' default answer is 'no'			
		previous_belief = self.beliefs[clue.about] 		
		after_update = previous_belief / (clue.value * clue.trustworthiness)
		# positive factor: the previous belief. If previous belief was high, to take it down will take some effort.
		# negative factor: the value of a clue: that is, the strength and direction of the clue.
		# 	the negative factor in turn is affected by the trustworthiness of he who formulated it.
		#	by logging these clues' execution, we can know when an Agent gave 'bad' feedback: that is, feedback that was
		#	later contradicted by many feedbacks on the opposite direction.
		
		self.beliefs[clue.about] = after_update		
		self.log(clue)
	
	def log(self,clue):
		"""
		Whenever a clue gets processed by God (or otherwise 'consumed'),
		we log it. Then we will be able to run analyses on the logs, so as
		to determine whether an agent (or algorithm) gave feedback
		whose effects (creation of a new link, downweighting of an algorithm
		or other agent) were appreciated (through more same-direction feedback)
		by other agents.
		"""
		with open('./clues.log','+a') as logs:
			logline = "[{}] : {}".format( ss.time.ctime(),str(clue))
			logs.write(str(logline))

	def handle_accuracy(self,clue):
		"""
		Handles clues about algorithms.
		"""
		
		algname = clue.about
		alg = [alg for alg in self.guardianangels if alg.name == algname]
		alg = alg[0]
		alg.receive(clue)
				
	def spawn_servants(self):
		"""
		Creates all GuardianAngels
		"""	

		self.guardianangels = []
		for algorithm in algs:
			GA = GuardianAngel(algorithm)
			GUARDIANANGELS.append(GA)
			self.guardianangels.append(GA)
		
	def consult_guardian_angels(self):
		"""
		Consults all guardian angels asking for their opinion about
		the whole network.
		Ideally, this function should be called at the beginning of
		the whole process only, or if weights get thoroughly screwed
		up.
		This function results in God re-virginating his belief states
		to a GuardianAngel-only informed belief state.
		
		Pleas note: computationally very heavy.
		
		"""
		
		opinions = {}
		
		for angel in self.guardianangels: # the list of opinions thus will be in the same order
			angel.evaluate_all()
			opinion = angel.evaluation
			
			for pair in opinion:
				judgement = opinion[pair]
				if not opinions.get(pair):
					opinions[pair] = []
				opinions[pair].append(opinion[pair])
		
		# now opinion is of list(dict( {frozenset({Cloud(),Cloud()}) : [ int() ] }  )) type. each dict is a frozenset({clouda,cloudb}) --> [0,1] mapping
		# 	for all links in the database; that is: all possible combinations of clouds.
				
		for pair in opinions:
			judgements = opinions[pair] # a vector of [0,1] floats
			for i in range(len(judgements)):
				judgement = judgements[i]
				author = self.guardianangels[i]
				newclue = Clue(pair,judgement,author)
			# now god formulates a clue on the guardian's behalf: this way we don't clog the guardian's logs
		
		self.consider_clues()

#	def suggest_link(self,link):
			
def init_base():
	global sky
	sky = ss.SemanticSky()
	global _god
	_god = God()
	
	





